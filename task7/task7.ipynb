{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=512 #大概需要2G的显存\n",
    "EPOCHS=20 # 总共训练批次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████▉| 9904128/9912422 [00:19<00:00, 931365.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                | 0/28881 [00:00<?, ?it/s]\n",
      " 57%|██████████████████▋              | 16384/28881 [00:00<00:00, 62057.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "  0%|                                              | 0/1648877 [00:00<?, ?it/s]\n",
      "\n",
      "  1%|▎                              | 16384/1648877 [00:00<00:28, 56688.81it/s]\n",
      "\n",
      "  3%|▉                              | 49152/1648877 [00:01<00:23, 68362.46it/s]\n",
      "\n",
      "  6%|█▊                             | 98304/1648877 [00:01<00:18, 84642.94it/s]\n",
      "\n",
      " 10%|██▉                          | 163840/1648877 [00:01<00:14, 105551.81it/s]\n",
      "\n",
      " 16%|████▌                        | 262144/1648877 [00:01<00:09, 143648.79it/s]\n",
      "\n",
      " 21%|██████                       | 344064/1648877 [00:01<00:07, 182939.38it/s]\n",
      "\n",
      " 27%|███████▊                     | 442368/1648877 [00:01<00:04, 241680.11it/s]\n",
      "\n",
      " 33%|█████████▌                   | 540672/1648877 [00:02<00:03, 311158.91it/s]\n",
      "\n",
      " 39%|███████████▏                 | 638976/1648877 [00:02<00:02, 390945.76it/s]\n",
      "\n",
      " 45%|████████████▉                | 737280/1648877 [00:02<00:01, 475776.51it/s]\n",
      "\n",
      " 51%|██████████████▋              | 835584/1648877 [00:02<00:01, 560027.27it/s]\n",
      "\n",
      " 57%|████████████████▍            | 933888/1648877 [00:02<00:01, 640517.61it/s]\n",
      "\n",
      " 63%|█████████████████▌          | 1032192/1648877 [00:02<00:00, 710623.39it/s]\n",
      "\n",
      " 69%|███████████████████▏        | 1130496/1648877 [00:02<00:00, 771398.19it/s]\n",
      "\n",
      " 75%|████████████████████▊       | 1228800/1648877 [00:02<00:00, 818469.81it/s]\n",
      "\n",
      " 80%|██████████████████████▌     | 1327104/1648877 [00:02<00:00, 857227.65it/s]\n",
      "\n",
      " 86%|████████████████████████▏   | 1425408/1648877 [00:03<00:00, 884223.62it/s]\n",
      "\n",
      " 92%|█████████████████████████▊  | 1523712/1648877 [00:03<00:00, 906658.02it/s]\n",
      "\n",
      " 98%|███████████████████████████▌| 1622016/1648877 [00:03<00:00, 923051.70it/s]\n",
      "\n",
      "1654784it [00:03, 509291.90it/s]                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "  0%|                                                 | 0/4542 [00:00<?, ?it/s]\n",
      "\n",
      "8192it [00:00, 14866.67it/s]                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9920512it [00:30, 931365.82it/s]                                               "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32768it [00:20, 62057.03it/s]                                                  "
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1=nn.Conv2d(1,10,5) # 10, 24x24\n",
    "        self.conv2=nn.Conv2d(10,20,3) # 128, 10x10\n",
    "        self.fc1 = nn.Linear(20*10*10,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x) #24\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)  #12\n",
    "        out = self.conv2(out) #10\n",
    "        out = F.relu(out)\n",
    "        out = out.view(in_size,-1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.251622\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.187996\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.096146\n",
      "\n",
      "Test set: Average loss: 0.0905, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.073066\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.074291\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.042246\n",
      "\n",
      "Test set: Average loss: 0.0505, Accuracy: 9836/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.029856\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.050501\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.050483\n",
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.021359\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.030007\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.041102\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.019379\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.041251\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.035415\n",
      "\n",
      "Test set: Average loss: 0.0310, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.022699\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.029975\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.021702\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.008535\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.022866\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.013525\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.010263\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.016467\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.016380\n",
      "\n",
      "Test set: Average loss: 0.0291, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.013087\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.009505\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.006662\n",
      "\n",
      "Test set: Average loss: 0.0258, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.005780\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.009607\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.028925\n",
      "\n",
      "Test set: Average loss: 0.0292, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.011019\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.004481\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.007062\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.004698\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.005283\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.007385\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.004302\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.003561\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.019719\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.002075\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.004062\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.002211\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.002723\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.004596\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.001970\n",
      "\n",
      "Test set: Average loss: 0.0373, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.001764\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.001957\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.000690\n",
      "\n",
      "Test set: Average loss: 0.0268, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.000749\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.001193\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.000204\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.001516\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.003704\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.003245\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.013608\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.004214\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.002746\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.010622\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.008566\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.011901\n",
      "\n",
      "Test set: Average loss: 0.0345, Accuracy: 9901/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
